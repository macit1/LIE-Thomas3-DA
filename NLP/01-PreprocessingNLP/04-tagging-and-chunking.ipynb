{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging and Chunking\n",
    "\n",
    "To help the machine understand a sentence, we will tell it what each word is.\n",
    "For that we use **P**art **O**f **S**peech tagging and **Chunking**.\n",
    "\n",
    "- [More info here](https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)\n",
    "\n",
    "\n",
    "## What is Part of Speech?\n",
    "\n",
    "\"The part of speech explains how a word is used in a sentence. There are 8 main POS tags: nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections.\"\n",
    "\n",
    "## How to do that?\n",
    "\n",
    "A lot of tools are performing this task. But SpaCy (again...) does it quite well. When you use the `nlp` object from it, it applies a complete preprocessing pipeline, including POS tagging.\n",
    "\n",
    "#### Let's practice: can you find the POS tag for each word using SpaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Tokenization\n",
      "Token: SpaCy\n",
      "Token: is\n",
      "Token: a\n",
      "Token: powerful\n",
      "Token: library\n",
      "Token: for\n",
      "Token: natural\n",
      "Token: language\n",
      "Token: processing\n",
      "Token: that\n",
      "Token: removes\n",
      "Token: stop\n",
      "Token: words\n",
      "Token: .\n",
      "Token: USA\n",
      "Token: ,\n",
      "Token: France\n",
      "Token: on\n",
      "Token: 21/10/2025\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "#spacy.load(\"en_core_web_sm\"): This loads a pre-trained spaCy language model. In this case, \"en_core_web_sm\" is a small English model trained on web text.\n",
    "#It includes components for tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"SpaCy is a powerful library for natural language processing that removes stop words. USA, France on 21/10/2025\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "doc = nlp(text)\n",
    "print(\"\\nStep 1: Tokenization\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Part-of-Speech Tagging\n",
      "Token: SpaCy, POS Tag: PROPN\n",
      "Token: is, POS Tag: AUX\n",
      "Token: a, POS Tag: DET\n",
      "Token: powerful, POS Tag: ADJ\n",
      "Token: library, POS Tag: NOUN\n",
      "Token: for, POS Tag: ADP\n",
      "Token: natural, POS Tag: ADJ\n",
      "Token: language, POS Tag: NOUN\n",
      "Token: processing, POS Tag: NOUN\n",
      "Token: that, POS Tag: PRON\n",
      "Token: removes, POS Tag: VERB\n",
      "Token: stop, POS Tag: VERB\n",
      "Token: words, POS Tag: NOUN\n",
      "Token: ., POS Tag: PUNCT\n",
      "Token: USA, POS Tag: PROPN\n",
      "Token: ,, POS Tag: PUNCT\n",
      "Token: France, POS Tag: PROPN\n",
      "Token: on, POS Tag: ADP\n",
      "Token: 21/10/2025, POS Tag: NUM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Part-of-Speech Tagging\n",
    "print(\"\\nStep 2: Part-of-Speech Tagging\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS Tag: {token.pos_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Named Entity Recognition (NER)\n",
      "Entity: USA, Label: GPE\n",
      "Entity: France, Label: GPE\n",
      "Entity: 21/10/2025, Label: DATE\n",
      "(USA, France, 21/10/2025)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Named Entity Recognition (NER)\n",
    "print(\"\\nStep 3: Named Entity Recognition (NER)\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Dependency Parsing\n",
      "Token: SpaCy, Dependency: nsubj, Head: is\n",
      "Token: is, Dependency: ROOT, Head: is\n",
      "Token: a, Dependency: det, Head: library\n",
      "Token: powerful, Dependency: amod, Head: library\n",
      "Token: library, Dependency: attr, Head: is\n",
      "Token: for, Dependency: prep, Head: library\n",
      "Token: natural, Dependency: amod, Head: language\n",
      "Token: language, Dependency: compound, Head: processing\n",
      "Token: processing, Dependency: pobj, Head: for\n",
      "Token: that, Dependency: nsubj, Head: removes\n",
      "Token: removes, Dependency: nsubj, Head: stop\n",
      "Token: stop, Dependency: relcl, Head: library\n",
      "Token: words, Dependency: dobj, Head: stop\n",
      "Token: ., Dependency: punct, Head: is\n",
      "Token: USA, Dependency: ROOT, Head: USA\n",
      "Token: ,, Dependency: punct, Head: USA\n",
      "Token: France, Dependency: appos, Head: USA\n",
      "Token: on, Dependency: prep, Head: USA\n",
      "Token: 21/10/2025, Dependency: pobj, Head: on\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Dependency Parsing\n",
    "print(\"\\nStep 4: Dependency Parsing\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Lemmatization\n",
      "Token: SpaCy, Lemma: SpaCy\n",
      "Token: is, Lemma: be\n",
      "Token: a, Lemma: a\n",
      "Token: powerful, Lemma: powerful\n",
      "Token: library, Lemma: library\n",
      "Token: for, Lemma: for\n",
      "Token: natural, Lemma: natural\n",
      "Token: language, Lemma: language\n",
      "Token: processing, Lemma: processing\n",
      "Token: that, Lemma: that\n",
      "Token: removes, Lemma: remove\n",
      "Token: stop, Lemma: stop\n",
      "Token: words, Lemma: word\n",
      "Token: ., Lemma: .\n",
      "Token: USA, Lemma: USA\n",
      "Token: ,, Lemma: ,\n",
      "Token: France, Lemma: France\n",
      "Token: on, Lemma: on\n",
      "Token: 21/10/2025, Lemma: 21/10/2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Lemmatization\n",
    "print(\"\\nStep 5: Lemmatization\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities:\n",
      "Entity: Apple Inc., Label: ORG\n",
      "Entity: Steve Jobs, Label: PERSON\n",
      "Entity: Steve Wozniak, Label: PERSON\n",
      "Entity: April 1976, Label: DATE\n",
      "Entity: iPhone, Label: ORG\n",
      "Entity: MacBook, Label: ORG\n",
      "\n",
      "Organizations mentioned in the text:\n",
      "- Apple Inc.\n",
      "- iPhone\n",
      "- MacBook\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in April 1976. The company is known for its innovative products like the iPhone and MacBook.\"\n",
    "\n",
    "# Process the text using the loaded model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display named entities and their labels\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "\n",
    "# Extract organizations from named entities\n",
    "organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "\n",
    "# Print information about organizations\n",
    "print(\"\\nOrganizations mentioned in the text:\")\n",
    "for org in organizations:\n",
    "    print(f\"- {org}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"I am a junior data scientist at Becode and my ultimate dream is to become a famous NLP engineer\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    pos = ## TO COMPLETE\n",
    "    print(token, \"--\", pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is chunking?\n",
    "\n",
    "\"Chunking is a process of extracting phrases from unstructured text. Instead of just simple tokens which may not represent the actual meaning of the text, its advisable to use phrases such as “South Africa” as a single word instead of ‘South’ and ‘Africa’ separate words.\"\n",
    "\n",
    "## How to do that?\n",
    "\n",
    "Well, every library has its own way of doing it. Let's see how SpaCy does it with [displacy, their vizaluazation tool](https://spacy.io/usage/visualizers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"\"\"In ancient Rome, some neighbors live in three adjacent houses. In the center is the house of Senex, who lives there with wife Domina, son Hero, and several slaves, including head slave Hysterium and the musical's main character Pseudolus.\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "doc = nlp(text)\n",
    "# Create a list of sentence\n",
    "sentence_spans = list(doc.sents)\n",
    "# Display SpaCy vizualizer for each sentence\n",
    "displacy.render(sentence_spans, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, search how SpaCy chunks the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text's chunking by using the Doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "* [Learning POS tagging & chunking in NLP](https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)\n",
    "* [Spacy API](https://spacy.io/api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
