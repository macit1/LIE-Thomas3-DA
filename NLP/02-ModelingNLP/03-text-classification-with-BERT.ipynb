{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS4troYKCHXW"
      },
      "source": [
        "# Text Classification with BERT\n",
        "\n",
        "![bert](https://res.cloudinary.com/practicaldev/image/fetch/s--ozy733MJ--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/q5e65ugnue96bir3usyk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5lJkWQCKqE"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a NLP model developed by Google in 2018. It is a model that is already pre-trained on a 2,5000M (+- 170 GB) words corpus from Wikipedia.\n",
        "\n",
        "To accomplish a particular NLP task, the pre-trained BERT model is used as a base and refined by adding an additional layer; the model can then be trained on a labeled dataset dedicated to the NLP task to be performed. This is the very principle of transfer learning. It is important to note that BERT is a very large model with 12 layers, 12 attention heads and 110 million parameters (BERT base).\n",
        "\n",
        "The BERT model is able to do :\n",
        "\n",
        "*   Translation\n",
        "*   Text generation\n",
        "*   Classification\n",
        "*   Question-answering\n",
        "*   ...\n",
        "\n",
        "### Why BERT?\n",
        "\n",
        "Using the General Language Understanding Evaluation ([GLUE](https://gluebenchmark.com/)) benchmark [leaderboard](https://gluebenchmark.com/leaderboard) , its easy to realize that many models on the list are all forks of BERT.\n",
        "\n",
        "## Let's go !\n",
        "To use BERT you need to have either pytorch or tensorflow installed in your environment. It is also preferable to have access to a GPU on your computer. If you don't have a GPU you can use [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "Next, let’s install the transformers package from Hugging Face. This package is an interface between BERT and pytorch and/or tensorflow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_111Kiv1_x7A",
        "outputId": "0b164f54-e57e-460f-96b1-758c4ebb48f5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IykzH85CDoUl"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "For this project we will use the data from Odile. Odile is a bot that tries to answer general questions on a few BeCode Discord servers. The sentences all come from conversations between learners and Odile on Discord.\n",
        "\n",
        "You'll find the data in `./dataset/odile_data.csv`. You can import them in a dataframe and display it.\n",
        "\n",
        "**Tip:** if you are using Google colab you can import the CSV in your google drive and connect your notebook to your Google drive (check on Google how to do that !)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "gJVn61y9_2Rm",
        "outputId": "d18a8a98-ce34-43fa-a66a-7b8fee9962b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1keVkxwE_R5"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "It's time to take a quick look at our data.\n",
        "\n",
        "As you see the questions from the learners are classified as intents (i.e. the goal the user has in mind when typing in a question or comment)\n",
        "\n",
        "**Exercise:** Use your data exploration and visualization skills to answer the the following questions:\n",
        "\n",
        "*   How many observations does the dataset contain?\n",
        "*   How many different labels does the dataset contain?\n",
        "*   Which labels contain the most observations?\n",
        "*   Which labels contain the fewest observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twAXoP2yFSKi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeVa52nFSpg"
      },
      "source": [
        "## It's time to clean up !\n",
        "\n",
        "\n",
        "Not all NLP tasks require the same preprocessing. In this case, we have to ask ourselves some questions: \n",
        "\n",
        "- Are there unwanted characters in the dataset? For example, do you want to keep the smiley's or not?  \n",
        "  - If, for example, you want to create labels to analyze feelings, it might be perishable to keep the smiley's.\n",
        "- Is it relevant to keep capital letters in sentences?\n",
        "  - In this case, capital letters don't really matter, because on one hand, not everyone starts their sentences with capital letters when chatting. On the other hand, the sentences are quite short, addressed directly to Odile. \n",
        "- Is it necessary to limit the number of characters in a sentence?\n",
        "  - Again in this case it may be preferable to limit the number of words. The questions asked to Odile are supposed to be short, as too long sentences could interfere with the classification if they contain too much information.\n",
        "\n",
        "There is no universal answer. Everything will depend on the expected result. \n",
        "\n",
        "**Exercise :** Clean the dataset.\n",
        "- Remove all unnecessary characters. You can choose to keep the smiley's or not.\n",
        "- Put all sentences in lower case.\n",
        "- Limit text to 256 words.\n",
        "\n",
        "What other preprocessing steps can you think of?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "Z8ZVkTmDGCVe",
        "outputId": "9dfcadfa-23ee-416b-9f9d-dbebbde195d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLc9G7nsGNir"
      },
      "source": [
        "## Defining observations (`X`) and labels (`y`)\n",
        "\n",
        "As you know, training a model requires a set of observations (`X`) and their corresponding labels (`y`).\n",
        "\n",
        "In that case, `X` is your clean text and `y` is the intent.\n",
        "\n",
        "Do not forget that we are dealing with a multi-class classification problem. Then, you may have to **one-hot encode** the target value. Keep track of the mapping between the one-hot encoding and the labels in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Cce6LYG8hU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirU5x74IJrg"
      },
      "source": [
        "## Split your dataset!\n",
        "\n",
        "After all this time, I dare to hope that it is not necessary to explain this step anymore!\n",
        "\n",
        "**Exercise :** Create the variables `X_train`, `X_val`, `X_test`, `y_train`, `y_val` and `y_test`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxit-YG0ou56"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m80bg5t0Jzq9"
      },
      "source": [
        "## Tokenization \n",
        "If you don't know what tokenization is anymore: look [here](../1.preprocessing/1.tokenization.ipynb).\n",
        "\n",
        "We will use the tokenizer provided by BERT. This is a pre-trained model that will save us time. \n",
        "\n",
        "**Exercise :** Create a `tokenizer` variable and instantiate `DistilBertTokenizer.from_pretrained()` from `transformers`. You have to load `distilbert-base-uncased` model. (Uncased for case-insensitive.) \n",
        "\n",
        "Read more: [Tokenizer documentation](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertTokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHma6LtqfLs",
        "outputId": "bf329bf5-40d2-41f8-f290-5b6bc9317881"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cQ19esLKBIK"
      },
      "source": [
        "### Tokenize the dataset\n",
        "\n",
        "Good! We have instantiated our tokenizer but we have not yet encoded our words in vector.\n",
        "To do this we will have to apply the tokenizer on our dataset. This will convert our texts into vectors.\n",
        "\n",
        "\n",
        "**Exercise:** Create the `train_encodings`, `val_encodings` and `test_encodings` by calling the tokenizer on `X_train`,  `X_val` and `X_test`.\n",
        "\n",
        "You need to know 3 parameters. \n",
        "\n",
        "- **max_length:** Maximum length of the sequence. You can set it to 200\n",
        "- **truncation:** This will truncate to a maximum length specified by the max_length argument. This will truncate token by token, removing a token from the longest sequence in the pair until the proper length is reached. You can set it to `True`\n",
        "- **padding:** this is the parameter to make all vectors have the same length. You can set it to `True`.\n",
        "\n",
        "[More info here](https://huggingface.co/docs/transformers/preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xITb1MwhKH8H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETZODcPViDKH"
      },
      "source": [
        "## Prepare the datasets for training\n",
        "\n",
        "You can now convert your training, evaluation and test sets in a dataset that will contain both observations and labels. Use the [from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) method from Tensorflow to create the datasets. This methods takes two arguments:\n",
        "\n",
        "*   The encodings that you have just created (casted as a `dict`)\n",
        "*   The labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyaj7R9ppBc7"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaazZtg3j0jE"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--J4LcUTkHjQ"
      },
      "source": [
        "### Load BERT model\n",
        "\n",
        "You will need to load the BERT pre-trained model by using the class `TFDistilBertForSequenceClassification`\n",
        "\n",
        "⚠️ You must use the same model as the one used for tokenization. So in our case  `distilbert-base-uncased`. \n",
        "\n",
        "* [BERT for Sequence Classification Documentation](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n",
        "\n",
        "**Exercise:** Create a model variable and load it by using  `TFDistilBertForSequenceClassification.from_pretrained()` As a parameter, you must indicate the number of labels (get this number from your original dataframe).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXIMee4HpNM-",
        "outputId": "4e6950d6-72ac-4872-fec2-1939ac59ea1f"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnPAcyWmykz"
      },
      "source": [
        "### Training arguments\n",
        "\n",
        "Let's define the the training arguments and compile our model\n",
        "\n",
        "*   Define the optimizer (Adam) and its learning rate\n",
        "*   Define the loss function that will be used (remember that we have one-hot encoded output data)\n",
        "*   Define the evaluation appropriate metrics\n",
        "*   Compile the model with the right metrics\n",
        "*   Display the model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5VYs6n6mpnZ",
        "outputId": "ecb07717-82eb-45ed-9530-4338901a0d36"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER =  \n",
        "LOSS = \n",
        "METRICS = \n",
        "\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGqIC2D12hjx"
      },
      "source": [
        "### Training\n",
        "\n",
        "Define first the number of epochs and the batch size for the training.\n",
        "\n",
        "The batch size will depend on your machine. If you have a weak GPU, I advise you to put 8 or 16.\n",
        "\n",
        "The number of epochs will depend on your machine, the batch size, etc...You can start with 5 for example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulixSczoHI0P"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E96Ad6lcpYdB",
        "outputId": "df318820-ff12-400e-b8c1-bee84123b49b"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset.batch(BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRfXFwZHInj4"
      },
      "source": [
        "### Plot the learning curve of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "vyh-V9Db36op",
        "outputId": "956ac673-1d15-47f1-ae70-50d27e859e10"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
        "    that is output from your `fit` method to plot the loss and accuracy of\n",
        "    the training and validation set.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
        "    axs[0].plot(history.history['accuracy'], label='training set')\n",
        "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
        "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, 1])\n",
        "\n",
        "    axs[1].plot(history.history['loss'], label='training set')\n",
        "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
        "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 10])\n",
        "    \n",
        "    axs[0].legend(loc='lower right')\n",
        "    axs[1].legend(loc='lower right')\n",
        "    \n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR0RWyeLIvQo"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "We can now evaluate our model on the test set. Use the `model.evaluate()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvHTZCujIyVt",
        "outputId": "b703e285-4b61-4633-b6f6-3402b80c861f"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset.batch(BATCH_SIZE))\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFB1j6bxa04"
      },
      "source": [
        "**Exercise:** is the accuracy the best metrics for this dataset ? Explain your answer !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AJ0X6FgJg2B"
      },
      "source": [
        "## Test your model\n",
        "\n",
        "Well done, you did it :-)\n",
        "\n",
        "Oh...I have an idea ! Try to classify the sentence *Well done !* with your model\n",
        "\n",
        "Think to apply all the preprocessing steps and predict the intent of the user.\n",
        "\n",
        "**Tip:** use the mapping you have created above to retrieve the original label of the prediction !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLdTyxKnBCej",
        "outputId": "f8b15540-52dd-4f83-acbf-318e36247e06"
      },
      "outputs": [],
      "source": [
        "text = \"Well done !\"\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1d59efe0a6ac6ca00d64bcda3954588e00cc9223d62022edd1b9de1af7efdfc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
